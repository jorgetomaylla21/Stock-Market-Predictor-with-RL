{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os \n",
    "import gym\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from vmdpy import VMD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a stock class to keep track of revenue and expenditures of stock over time. \n",
    "This class was generated by ChatGPT to have a better mental organization on how to keep track of profits over time in the reinforcement learning training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bought 10 shares at $150.0 each.\n",
      "Sold 5 shares at $160.0 each.\n",
      "Current portfolio value: $10150.0\n"
     ]
    }
   ],
   "source": [
    "class Portfolio:\n",
    "    def __init__(self, initial_cash):\n",
    "        self.cash = initial_cash\n",
    "        self.stock_quantity = 0\n",
    "        #add the current portfolio value\n",
    "\n",
    "        #consider adding a crashed flag(went below 0) or a can't buy so skip the buy action, same with sell\n",
    "\n",
    "    def buy_stock(self, price, quantity):\n",
    "        cost = price * quantity\n",
    "        if cost <= self.cash:\n",
    "            self.cash -= cost\n",
    "            self.stock_quantity += quantity\n",
    "            print(f\"Bought {quantity} shares at ${price} each.\")\n",
    "        else:\n",
    "            #skip the buy action\n",
    "            print(\"Insufficient funds to buy the stock.\")\n",
    "\n",
    "    def sell_stock(self, price, quantity):\n",
    "        if self.stock_quantity >= quantity:\n",
    "            revenue = price * quantity\n",
    "            self.cash += revenue\n",
    "            self.stock_quantity -= quantity\n",
    "            print(f\"Sold {quantity} shares at ${price} each.\")\n",
    "        else:\n",
    "            #skip the sell action\n",
    "            print(\"Insufficient shares to sell.\")\n",
    "\n",
    "    def get_portfolio_value(self, current_price):\n",
    "        return self.cash + current_price * self.stock_quantity\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "initial_cash = 10000\n",
    "portfolio = Portfolio(initial_cash)\n",
    "\n",
    "# Buying stocks\n",
    "portfolio.buy_stock( 150.0, 10)\n",
    "\n",
    "# Selling some stocks\n",
    "portfolio.sell_stock( 160.0, 5)\n",
    "\n",
    "# Getting the current portfolio value\n",
    "current_price = 170\n",
    "portfolio_value = portfolio.get_portfolio_value(current_price)\n",
    "print(f\"Current portfolio value: ${portfolio_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the dimensions and gymnasium spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9081.06"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Discrete(3)\n",
    "Dict({0:Box(0,initial_cash, shape=(1,)), 1:Box(0,initial_cash, shape=(1,))}).sample()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the RL environment. Must inherit from the gymnasium environment class. Checkout online documentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Environment\n",
    "class StockTradingEnvironment(gymnasium.Env):\n",
    "    def __init__(self, actual_price_data, predicted_price_data, sentiment_data, initial_cash):\n",
    "        super(StockTradingEnvironment, self).__init__()\n",
    "\n",
    "        # Initialize environment variables\n",
    "        self.initial_cash = initial_cash\n",
    "        self.actual_price_data = actual_price_data\n",
    "        self.predicted_price_data = predicted_price_data                \n",
    "        self.sentiment_data = sentiment_data\n",
    "        self.price_index, self.sentiment_index = 0, 0\n",
    "        self.action_space = Discrete(3)  # buy, sell, hold\n",
    "        self.action_range = Dict({\"Buy\":Box(0,initial_cash, shape=(1,)), \"Sell\":Box(0,initial_cash, shape=(1,))}) #actual stock ranges\n",
    "        self.observation_space = Dict({'actual_price':Box(0, 500, shape=(1,)),'predicted_prices':Box(0, 500, shape=(1,)), \"sentiment\":Box(-1, 1, shape=(1,))})\n",
    "\n",
    "        self.state = {\n",
    "        'actual_price' : np.array([self.actual_price_data[0]], dtype=np.float32),\n",
    "        'predicted_prices': np.array([self.predicted_price_data[0]], dtype=np.float32),  # Ensure it's a float32 numpy array\n",
    "        'sentiment': np.array([self.sentiment_data[0]], dtype=np.float32)     # Ensure it's a float32 numpy array\n",
    "        }\n",
    "        # predicted stock states\n",
    "        self.cash = initial_cash\n",
    "        self.stock_quantity = 0\n",
    "        self.current_value = 0\n",
    "        self.seed = 0\n",
    "        \n",
    "\n",
    "    def buy_stock(self, price, desired_amount):\n",
    "        quantity = round(desired_amount / price, 1)\n",
    "        cost = price * quantity\n",
    "        if cost <= self.cash:\n",
    "            self.cash -= cost\n",
    "            self.stock_quantity += quantity\n",
    "            #print(f\"Bought {quantity} shares at ${price} each.\")\n",
    "        else:\n",
    "            #print(\"Insufficient funds to buy the stock.\")\n",
    "            return False\n",
    "        #get portfolio value based off real price\n",
    "        self.current_value = self.get_portfolio_value(self.state['actual_price'][0])\n",
    "        return True\n",
    "    def sell_stock(self, price, desired_amount):\n",
    "        quantity = round(desired_amount / price, 1)\n",
    "        if self.stock_quantity >= quantity:\n",
    "            revenue = price * quantity\n",
    "            self.cash += revenue\n",
    "            self.stock_quantity -= quantity\n",
    "            #print(f\"Sold {quantity} shares at ${price} each.\")\n",
    "        else:\n",
    "            #print(\"Insufficient shares to sell.\")\n",
    "            return False\n",
    "        self.current_value = self.get_portfolio_value(self.state['actual_price'][0])\n",
    "        return True\n",
    "\n",
    "    def get_portfolio_value(self, current_price):\n",
    "        return self.cash + current_price * self.stock_quantity\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=4)\n",
    "        # Reset environment to initial state\n",
    "        self.cash = self.initial_cash\n",
    "        self.stock_quantity = 0\n",
    "        self.current_value = 0\n",
    "        self.price_index = 0\n",
    "        self.sentiment_index = 0\n",
    "        self.state = {\n",
    "        'actual_price' : np.array([self.actual_price_data[0]], dtype=np.float32),\n",
    "        'predicted_prices': np.array([self.predicted_price_data[0]], dtype=np.float32),  # Ensure it's a float32 numpy array\n",
    "        'sentiment': np.array([self.sentiment_data[0]], dtype=np.float32)     # Ensure it's a float32 numpy array\n",
    "        }\n",
    "\n",
    "        info = {}\n",
    "        return self.state, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Execute one step within the environment\n",
    "        # Update state based on action taken\n",
    "        # Calculate reward\n",
    "        # Return next observation, reward, done flag, and additional info\n",
    "        old_value = self.current_value\n",
    "        if action == 0:\n",
    "            #we buy based on predicted stock\n",
    "            buy_successful = self.buy_stock(self.state['predicted_prices'][0], self.action_range.sample()[\"Buy\"][0])\n",
    "            \n",
    "        elif action == 1:\n",
    "            sell_successful = self.sell_stock(self.state['predicted_prices'][0], self.action_range.sample()[\"Sell\"][0])\n",
    "        #else the action is a \"Hold\" so we do nothing (we can adjust to reduce the reward if we hold)\n",
    "        #now we calculate the reward based on portfolio value\n",
    "\n",
    "        #if we sell more than we have or buy more than we can afford, punish the action\n",
    "        if (action == 0 and not buy_successful) or (action == 1 and not sell_successful):\n",
    "            reward = -10\n",
    "        else:\n",
    "            reward = self.current_value - old_value\n",
    "        \n",
    "        done = self.price_index >= len(self.predicted_price_data) - 1\n",
    "        self.price_index += 1\n",
    "        self.sentiment_index += 1\n",
    "        truncated = False\n",
    "        if not done:\n",
    "            self.state = {\n",
    "                'actual_price':np.array([self.actual_price_data[self.price_index]], dtype=np.float32),\n",
    "                'predicted_prices': np.array([self.predicted_price_data[self.price_index]], dtype=np.float32), \n",
    "                'sentiment': np.array([self.sentiment_data[self.sentiment_index]], dtype=np.float32)\n",
    "            }\n",
    "        info = {}\n",
    "        return self.state, reward, done, truncated, info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data and do VMD\n",
    "frames = []\n",
    "for i in range(1, 13):\n",
    "    i_str = str(i)\n",
    "    if i % 10 == i:\n",
    "        i_str = '0'+i_str\n",
    "    frames.append(pd.read_csv(f'train_data/60min_MSFT_2016_{i_str}.csv', index_col='timestamp').iloc[::-1])\n",
    "for i in range(1, 13):\n",
    "    i_str = str(i)\n",
    "    if i % 10 == i:\n",
    "        i_str = '0'+i_str\n",
    "    frames.append(pd.read_csv(f'train_data/60min_MSFT_2017_{i_str}.csv', index_col='timestamp').iloc[::-1])\n",
    "\n",
    "for i in range(1, 2):\n",
    "    i_str = str(i)\n",
    "    if i % 10 == i:\n",
    "        i_str = '0'+i_str\n",
    "    frames.append(pd.read_csv(f'train_data/60min_MSFT_2018_{i_str}.csv', index_col='timestamp').iloc[::-1])\n",
    "stockprices = pd.concat(frames)\n",
    "\n",
    "alpha = 5000      # moderate bandwidth constraint  \n",
    "tau = 0           # noise-tolerance (no strict fidelity enforcement)  \n",
    "K = 5              # 5 modes  \n",
    "DC = 0             # no DC part imposed  \n",
    "init = 1           # initialize omegas uniformly  \n",
    "tol = 1e-7\n",
    "\n",
    "signals, u_hat, omega = VMD(stockprices['close'].to_numpy(), alpha, tau, K, DC, init, tol)\n",
    "stockprices.drop(stockprices.tail(1).index,inplace=True)\n",
    "for i, signal in enumerate(signals):\n",
    "    stockprices[f'signal{i}'] = signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = tf.keras.models.load_model('./trained_models/LSTM_Price_Predictor_Trial.keras')\n",
    "scaler = StandardScaler()\n",
    "window_size = 50\n",
    "future_steps = 5\n",
    "\n",
    "def preprocess_data(data=stockprices, scaler=scaler, window_size=window_size):\n",
    "    raw = data[['close','signal0', 'signal1', 'signal2', 'signal3', 'signal4']][-window_size:].values\n",
    "    raw = scaler.fit_transform(raw)\n",
    "    X_test = [raw[i-window_size:i, 1:] for i in range(window_size, raw.shape[0])]\n",
    "    X_test = np.array(X_test)\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    return X_test\n",
    "\n",
    "X_test = preprocess_data()\n",
    "predicted_price_ = model.predict(X_test)\n",
    "# Post-processing to fit scaler expectations\n",
    "full_dummy_features = np.zeros((predicted_price_.shape[0], 6))  # Create a dummy array with the same number of columns as the scaler expects\n",
    "full_dummy_features[:, 0] = predicted_price_.ravel()  # Assuming 'close' is the first column\n",
    "\n",
    "# Inverse transform\n",
    "predicted_price = scaler.inverse_transform(full_dummy_features)[:, 0]  # Inverse transform and select only the 'close' column\n",
    "\n",
    "stockprices.loc[:future_steps-1, \"Predictions_lstm\"] = None\n",
    "stockprices.loc[future_steps:, \"Predictions_lstm\"] = predicted_price[:-future_steps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space is discrete and of size 3 (buy, sell, hold), so we can use A2C, PPo, and DQN algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RL Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Initialize agent variables\n",
    "        pass\n",
    "    \n",
    "    def act(self, state):\n",
    "        # Choose action based on current state\n",
    "        pass\n",
    "    \n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        # Update agent's Q-values based on experience\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "state_size = len(price_data.columns) + len(sentiment_data.columns)\n",
    "action_size = 3\n",
    "batch_size = 32\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and agent\n",
    "# env = StockTradingEnvironment(price_data, sentiment_data)\n",
    "# agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# # Training loop\n",
    "# for episode in range(num_episodes):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "#     while not done:\n",
    "#         action = agent.act(state)\n",
    "#         next_state, reward, done, _ = env.step(action)\n",
    "#         agent.train(state, action, reward, next_state, done)\n",
    "#         state = next_state\n",
    "#         total_reward += reward\n",
    "#     print(f'Episode: {episode}, Total Reward: {total_reward}')\n",
    "\n",
    "# Evaluate the trained agent\n",
    "# Evaluate performance on validation dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALL CSV FILE TO LOAD PRICE PREDICTION AND SENTIMENT   \n",
    "data = pd.read_csv(\"all_data.csv\")\n",
    "predicted_price_data = data.iloc[5:, 7].values\n",
    "sentiment_data = data.iloc[:-5, 6].values\n",
    "actual_price_data = data.iloc[:-5, 4].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StockTradingEnvironment(actual_price_data[:3000], predicted_price_data[:3000], sentiment_data[:3000], 10000)\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:1 Score:310.4903264244531\n",
      "\n",
      "EPISODE:2 Score:1677.2659228619068\n",
      "\n",
      "EPISODE:3 Score:689.4063104679626\n",
      "\n",
      "EPISODE:4 Score:1824.3748923028907\n",
      "\n",
      "EPISODE:5 Score:2239.437879041736\n",
      "\n",
      "EPISODE:6 Score:1745.225472805516\n",
      "\n",
      "EPISODE:7 Score:1485.9139710785694\n",
      "\n",
      "EPISODE:8 Score:681.7843872061949\n",
      "\n",
      "EPISODE:9 Score:753.389840590833\n",
      "\n",
      "EPISODE:10 Score:996.5993059100638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, truncated, info = env.step(action)\n",
    "        score+=reward\n",
    "        #print(\"state : \", n_state)\n",
    "    print('EPISODE:{} Score:{}\\n'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to Training\\Logs\\PPO_12\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1241 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 2.28e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 783         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006915222 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -1e-05      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.89e+03    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    value_loss           | 1.41e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.65e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 674         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011903944 |\n",
      "|    clip_fraction        | 0.0594      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -1.67e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1e+04       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00651    |\n",
      "|    value_loss           | 1.78e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.65e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 643         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008368329 |\n",
      "|    clip_fraction        | 0.016       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.05e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00279    |\n",
      "|    value_loss           | 6.79e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.7e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 623         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010280331 |\n",
      "|    clip_fraction        | 0.0631      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.97e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00584    |\n",
      "|    value_loss           | 1.14e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3e+03        |\n",
      "|    ep_rew_mean          | 1.46e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 599          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067186365 |\n",
      "|    clip_fraction        | 0.0185       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.05e+03     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00371     |\n",
      "|    value_loss           | 1.43e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.46e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 598         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006155143 |\n",
      "|    clip_fraction        | 0.0582      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.28e+04    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00177     |\n",
      "|    value_loss           | 1.1e+05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.19e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 595         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010803949 |\n",
      "|    clip_fraction        | 0.0164      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 1.01e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.46e+03    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00389    |\n",
      "|    value_loss           | 1.53e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3e+03        |\n",
      "|    ep_rew_mean          | 1.28e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 592          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040137246 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.45e+03     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000359    |\n",
      "|    value_loss           | 6.32e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.28e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 592         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011134341 |\n",
      "|    clip_fraction        | 0.0375      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.971      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.43e+03    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00297    |\n",
      "|    value_loss           | 1.09e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3e+03        |\n",
      "|    ep_rew_mean          | 1.17e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 593          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040582856 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.961       |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.63e+03     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.000298    |\n",
      "|    value_loss           | 1.4e+04      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.21e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 592         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012147974 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.01e+05    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00409    |\n",
      "|    value_loss           | 1.8e+05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.21e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004719084 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.4e+04     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.000374    |\n",
      "|    value_loss           | 1.12e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3e+03      |\n",
      "|    ep_rew_mean          | 1.24e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 595        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 48         |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03449018 |\n",
      "|    clip_fraction        | 0.0263     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.882     |\n",
      "|    explained_variance   | 2.38e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.7e+03    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | 0.00268    |\n",
      "|    value_loss           | 1.76e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.22e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 596         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018107489 |\n",
      "|    clip_fraction        | 0.00767     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.796      |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.25e+03    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00349    |\n",
      "|    value_loss           | 5.94e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.22e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 592         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011809178 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.8        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.03e+03    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.00545     |\n",
      "|    value_loss           | 6.34e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.49e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 591         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011585779 |\n",
      "|    clip_fraction        | 0.0381      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.751      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.31e+03    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.000262    |\n",
      "|    value_loss           | 1.14e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 1.74e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 593         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000256897 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.2e+03     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00107    |\n",
      "|    value_loss           | 1.08e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3e+03      |\n",
      "|    ep_rew_mean          | 1.74e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 594        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 65         |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00773657 |\n",
      "|    clip_fraction        | 0.0208     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.605     |\n",
      "|    explained_variance   | 2.38e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.52e+03   |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.00476   |\n",
      "|    value_loss           | 1.45e+05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3e+03       |\n",
      "|    ep_rew_mean          | 2.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 596         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005028962 |\n",
      "|    clip_fraction        | 0.00278     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.000134    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.22e+04    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    value_loss           | 1.6e+04     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3e+03        |\n",
      "|    ep_rew_mean          | 2.43e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 597          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006314805 |\n",
      "|    clip_fraction        | 0.0613       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.518       |\n",
      "|    explained_variance   | 1.57e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.76e+03     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | 0.00278      |\n",
      "|    value_loss           | 2.94e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3e+03         |\n",
      "|    ep_rew_mean          | 2.78e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 598           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 75            |\n",
      "|    total_timesteps      | 45056         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0402466e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.521        |\n",
      "|    explained_variance   | 0.000385      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.97e+05      |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.000291     |\n",
      "|    value_loss           | 6.88e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3e+03         |\n",
      "|    ep_rew_mean          | 2.78e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 600           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 78            |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9033556e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.519        |\n",
      "|    explained_variance   | 0.00132       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.63e+03      |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -4.65e-05     |\n",
      "|    value_loss           | 6.17e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3e+03         |\n",
      "|    ep_rew_mean          | 3.08e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 600           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 81            |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1089636e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.514        |\n",
      "|    explained_variance   | 0.00272       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.33e+03      |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -6.74e-05     |\n",
      "|    value_loss           | 9.41e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 3e+03         |\n",
      "|    ep_rew_mean          | 3.36e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 601           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 85            |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.8781235e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.514        |\n",
      "|    explained_variance   | 0.000444      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.15e+05      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | 1.39e-05      |\n",
      "|    value_loss           | 3.8e+05       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.16.1\n",
      "Uninstalling tensorflow-2.16.1:\n",
      "  Would remove:\n",
      "    c:\\users\\jorge tomaylla\\anaconda3\\lib\\site-packages\\tensorflow-2.16.1.dist-info\\*\n",
      "Proceed (Y/n)? \n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=50000)\n",
    "model.save('PPO2')\n",
    "#evaluate_policy(model, env, n_eval_episodes=10, render=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
